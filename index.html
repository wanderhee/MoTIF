<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57pOZl+62Q+FPKCa20HHFD+pC+pCr1+aI/gWFEudZo+cWI1" crossorigin="anonymous">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
  <body>
    <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Kemo Jiang</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Xuan Cai</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhiyong Cui</a><sup>*,1</sup>,</span>
                    <span class="author-block">
                    <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Aoyong Li</a><sup>1</sup>,</span>
                      <span class="author-block">
                    <a href="FIFTH AUTHOR PERSONAL LINK" target="_blank">Yilong Ren</a><sup>1,2</sup>,</span>
                        <span class="author-block">
                    <a href="SIXTH AUTHOR PERSONAL LINK" target="_blank">Haiyang Yu</a><sup>1,2</sup>,</span>
                          <span class="author-block">
                    <a href="SEVENTH AUTHOR PERSONAL LINK" target="_blank">Hao Yang</a><sup>3</sup>,</span>
                            <span class="author-block">
                    <a href="EIGHTH AUTHOR PERSONAL LINK" target="_blank">Daocheng Fu</a><sup>4</sup>,</span>
                              <span class="author-block">
                    <a href="EIGHTH AUTHOR PERSONAL LINK" target="_blank">Licheng Wen</a><sup>4</sup>,</span>
                                <span class="author-block">
                    <a href="EIGHTH AUTHOR PERSONAL LINK" target="_blank">Pinlong Cai</a><sup>*,4</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Beihang University, 100191, Beijing, P.R.China.,</span>
                    <span class="author-block"><sup>2</sup>Zhongguancun National Laboratory, 100191, Beijing, P.R.China. </span>
                    <span class="eql-cntrb"><small><br><sup></sup>First author: jiangkemou@buaa.edu.cn</small></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author: zhiyongc@buaa.edu.cn, yilongren@buaa.edu.cn</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/KoMA.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/jkmhhh/KoMA_Code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

    <!-- on-ramp video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/test_exp_1_4_2-episode-0.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        The scenario of merging onto the highway from an entrance ramp.
      </h2>
    </div>
  </div>
</section>
<!-- End on-ramp video -->

<!-- Paper abstract -->
<section class="abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) as autonomous agents offer a novel avenue for tackling real-world challenges through a knowledge-driven manner. 
            These LLM-enhanced methodologies excel in generalization and interpretability. 
            However, the complexity of driving tasks often necessitates the collaboration of multiple, heterogeneous agents, underscoring the need for such LLM-driven agents to engage in cooperative knowledge sharing and cognitive synergy.
            Despite the promise of LLMs, current applications predominantly center around single-agent scenarios, which limits their scope in the face of intricate, interconnected tasks. 
            To broaden the horizons of knowledge-driven strategies and bolster the generalization capabilities of autonomous agents, we propose the KoMA framework consisting of the multi-agent interaction, the multi-step planning, the shared-memory, and the ranking-based reflection modules to enhance multi-agents' decision-making in complex driving scenarios.
            Based on the framework's generated  text descriptions of driving scenarios, the multi-agent interaction module enables LLM agents to analyze and infer the intentions of surrounding vehicles based on scene information , akin to human cognition. 
            The multi-step planning module enables LLM agents to analyze and obtain final action decisions layer by layer to ensure consistent goals for short-term action decisions. 
            The shared memory module can accumulate collective experience to make superior decisions, and the ranking-based reflection module can evaluate and improve agent behavior with the aim of enhancing driving safety and efficiency. 
            The KoMA framework not only enhances the robustness and adaptability of autonomous driving agents but also significantly elevates their generalization capabilities across diverse scenarios.
            Empirical results demonstrate the superiority of our approach over traditional methods, particularly in its ability to handle complex, unpredictable driving environments without extensive retraining.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-light is-small">
  <div class="hero-body">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h3 class="title is-3" style="text-align: center; padding-bottom: 5px; color: rgb(63	172	38)"><bold>Overview of the KoMA framework</bold></h3>
      </div>
    </div>
  </div>
  </div>
</section>

<!-- Method Overview -->
<section class="Overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method Overview</h2>
          <center>
          <img src="static/images/framework.png" alt="Framework" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
               We investigate the potential of leveraging LLMs within a multi-agent framework to enhance the decision-making capabilities of autonomous driving system. This study aims to develop a multi-agent autonomous driving decision-making framework based on large language model, which simulates human decision-making process and combines LLM's own reasoning ability to form knowledge-driven, so as to improve the safety and efficiency of LLM agents driving in relatively complex environments. The framework consists of five complete modules: environment module, multi-agent interaction module, multi-step planning module, shared-memory module and ranking-based reflection module. The specific research contents are as follows:
              (1) Environment module: Create a realistic autonomous driving simulation scenario for highway ramp merging, and convert the simulation images and data into textual descriptions of the scene to serve as part of the input for the Large Language Model.
              (2) Multi-agent interaction module:By meticulously analyzing the historical behaviors and real-time status information of other vehicles to infer their potential intentions, the intelligent agent can formulate a series of subsequent action plans, achieving implicit interactions similar to those of human drivers.
              (3) Multi-step planning module: build a three-layer progressive thinking chain of goal - plan - action, so that the LLM can reason and think step by step and layer for complex scenes, and get the final action decision.
              (4) Shared-memory module: Develop a unified vector database accessible to all Large Language Model (LLM) agents, ensuring consistency in experience and performance. This approach is akin to the parameter-sharing mechanism used among multiple agents in reinforcement learning, which achieves similar benefits.
              (5) Ranking-based reflection module: By employing specific metrics to quantify the safety and efficiency of the vehicle's state after each action decision is executed, the reflection agent reflects on and revises low-scoring erroneous decisions after the scenario concludes. Finally, the revised decisions, along with high-scoring ones, are stored in the shared memory module for collective learning and improvement.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Prompt Engineering -->
<section class="Prompt Engineering ">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Reasoning process</h2>
          <center>
          <img src="static/images/reasoning.png" alt="Reasoning" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
               The multi-step planning module is a chain of thought(CoT) to guide LLM make the final action decision.LLM firstly analyzes the goal according to the current scenario, then makes the plan, and finally makes the action decision. 
              This structured planning process enables the LLM agent to maintain a clear goal for its actions and more effectively pursue long-term goals. Based on the textual description of the current scene and the experiential playback of historical similar scenes, the LLM finally selects an action decision through continuous analysis and then returns the action decision to the environment module for execution.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h3 class="title is-3" style="text-align: center; padding-bottom: 5px; color: rgb(63	172	38)"><bold>Other scenarios for testing generalization capabilities.</bold></h3>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/test_GE_1_8-episode-0.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Decrease the number of lanes on the main roadway
      </h2>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/test_GE1_5-episode-0.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Increase the number of lanes on the main roadway
      </h2>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container my-small-container">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/roundabout_5-episode-0.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Roundabout
      </h2>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{jiang2024koma,
  title={Koma: Knowledge-driven multi-agent framework for autonomous driving with large language models},
  author={Jiang, Kemou and Cai, Xuan and Cui, Zhiyong and Li, Aoyong and Ren, Yilong and Yu, Haiyang and Yang, Hao and Fu, Daocheng and Wen, Licheng and Cai, Pinlong},
  journal={IEEE Transactions on Intelligent Vehicles},
  year={2024},
  publisher={IEEE}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Acknowledgements: The authors would like to appreciate the financial support
            of the National Natural Science Foundation of China (project number:52202378), 
            Beijing Natural Science Foundation (project number: L243008), 
            the Open Research Project Program of the State Key Laboratory of Internet of Things for Smart City (project number: SKL-IoTSC(UM)-2021-2023/ORP/GA08/2022), 
            and the Ministry of Transport of PRC Key Laboratory of Transport Industry of Comprehensive Transportation Theory (Grant No. MTF2023002).
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
    
</body>
</html>
