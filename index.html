<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MoTIF: An end-to-end Multimodal Road Traffic Scene Understanding Foundation Model</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57pOZl+62Q+FPKCa20HHFD+pC+pCr1+aI/gWFEudZo+cWI1" crossorigin="anonymous">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
  <body>
    <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MoTIF: An end-to-end Multimodal Road Traffic Scene Understanding Foundation Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Zihe Wang</a><sup>a,c</sup>,</span>
                <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Changxin Chen </a><sup>a,c</sup>,</span>
                  <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhiyong Cui</a><sup>*,a,c</sup>,</span>
                  <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">ufeng Bi</a><sup>b,c</sup>,</span>
                <span class="author-block">
                <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Yilong Ren</a><sup>a,c</sup>,</span>
                  <span class="author-block">
                <a href="FIFTH AUTHOR PERSONAL LINK" target="_blank">Zijian Wang</a><sup>b,c</sup>,</span>
                    <span class="author-block">
                <a href="SIXTH AUTHOR PERSONAL LINK" target="_blank">Haiyang Yu</a><sup>*,a,c</sup>,</span>
                      <span class="author-block">
                <a href="SEVENTH AUTHOR PERSONAL LINK" target="_blank">Delan Kong</a><sup>c,d</sup>,</span>
                        <span class="author-block">
                <a href="EIGHTH AUTHOR PERSONAL LINK" target="_blank">Shoutong Yuan</a><sup>e</sup>,</span>
                          <span class="author-block">
                <a href="EIGHTH AUTHOR PERSONAL LINK" target="_blank">Zhiqiang Lie</a><sup>e</sup>,</span>
                            <span class="author-block">
                <a href="EIGHTH AUTHOR PERSONAL LINK" target="_blank">Zhengyang Yue</a><sup>e</sup>,</span>
                            <span class="author-block">
                <a href="EIGHTH AUTHOR PERSONAL LINK" target="_blank">Yinhai Wang</a><sup>f</sup>.</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>a</sup>Beihang University, 100191, Beijing, P.R.China.,</span>
                    <span class="author-block"><sup>b</sup>Shandong Hi-speed Group Co., Ltd, 250098, Jinan, P.R.China. </span>
                    <span class="author-block"><sup>c</sup>State Key Laboratory of Intelligent Transportation System, 100191, Beijing, P.R.China. </span>
                    <span class="author-block"><sup>d</sup>Shandong Hi-Speed Information Group, 2500113, Jinan, P.R.China.,</span>
                    <span class="author-block"><sup>e</sup>CATARC Intelligent and Connected Technology Co.,Ltd., 300380, Tianjing, P.R.China. </span>
                    <span class="author-block"><sup>f</sup>Civil and Environmental Engineering, University of Washington, WA 98195, Seattle, U.S.American.</span>
                    <span class="eql-cntrb"><small><br><sup></sup>First author: by2313310@buaa.edu.cn</small></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author: zhiyongc@buaa.edu.cn, hyyu@buaa.edu.cn</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/MoTIF.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/wanderhee/MoTIF-Datasets-Code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- on-ramp video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here -->
        <!-- <source src="static/videos/test_exp_1_4_2-episode-0.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        The scenario of merging onto the highway from an entrance ramp.
      </h2>
    </div>
  </div>
</section> -->
<!-- End on-ramp video -->

<!-- Paper abstract -->
<section class="abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video-based road intelligent detection constitutes a critical component in modern intelligent transportation systems, serving as a crucial enabler for comprehensive transportation planning and emergency traffic management. Current traffic scene perception methodologies relying on conventional deep learning architectures present inherent limitations, including heavy dependence on extensive manual annotations of traffic elements and predefined rule configurations. These approaches demonstrate constrained semantic representation capacity and limited generalizability across heterogeneous traffic scenarios. To address these challenges, this paper proposes a novel end-to-end Multimodal Foundation Model (MFM) architecture that jointly generates dynamic traffic event detection outcomes and semantic-rich contextual descriptions. Through integration of Low-Rank Adaptation (LoRA) as a parameter-efficient fine-tuning strategy, we develop the Multimodal Road Traffic Scene Understanding Foundation Model (MoTIF), which establishes cross-modal alignment between visual patterns and textual semantics. This framework demonstrates enhanced capability in extracting salient traffic targets and generating hierarchical scene representations, significantly improving automated detection efficiency in road video analytics. Notably, MoTIF exhibits contextual reasoning capabilities for implicit traffic event interpretation. Extensive evaluations on two real-world datasets encompassing urban road intersection scenarios in Tianjin and highway monitoring systems in Shandong Province reveal that MoTIF achieves superior performance metrics: 65.81 average score on multimodal scene understanding assessment and 84.17\% event detection accuracy, outperforming mainstream benchmarks in both precision and computational efficiency. This research advances multimodal learning paradigms for intelligent transportation systems while providing practical insights for adaptive traffic management applications. The dataset concluding Tianjin road intersection surveillance video and corresponding data annotation is published on https://github.com/wanderhee/MoTIF-Datasets.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-light is-small">
  <div class="hero-body">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h3 class="title is-3" style="text-align: center; padding-bottom: 5px; color: rgb(63	172	38)"><bold>Main Contributions</bold></h3>
      </div>
    </div>
  </div>
  </div>
</section>

<!-- Main Contributions -->
<section class="Overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
          <div class="level-set has-text-justified">
            <p class="contribution-paragraph">
              Our contributions can be mainly divided into the following parts:<br>

              <b>1. Model:</b> The research in this paper focuses on improving the recognition ability of MFMs in the complex environment of traffic roads. The model proposed in this paper fine-tunes the various types of events in road traffic, which ensures the model's ability to accurately recognize and reason in road traffic scenarios.<br>

              <b>2. Task:</b> MoTIF outputs structured descriptive text, which enables the detection and analysis of various types of traffic events. The specific inference ability of the multi-modal large model makes it possible to further deduce the traffic state.<br>

              <b>3. Dataset:</b> This paper constructs a multimodal dataset for video understanding of road traffic. We propose a set of automated video annotation methods for traffic target object detection and semantic segmentation fusion for highways. We use Q-A question and answer pairs to annotate roadside surveillance videos of highways.<br>

              <b>4. Benchmark:</b> This work develops a multimodal video analysis benchmark tailored to diverse tasks. For intersection scene understanding, it employs NLG metrics including BLEU-4, ROUGE-L, CIDEr, and semantic relevance metrics such as BERTScore, alongside structured human evaluation. For traffic event detection, classification metrics — Accuracy, Precision, and F1-Score — are utilized.
            </p>
          </div>
      </div>
    </div>
  </div>


<section class="hero is-light is-small">
  <div class="hero-body">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h3 class="title is-3" style="text-align: center; padding-bottom: 5px; color: rgb(63	172	38)"><bold>Framework Overview of the MoTIF</bold></h3>
      </div>
    </div>
  </div>
  </div>
</section>

<!-- Method Overview -->
<section class="Overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <!-- <h2 class="title is-3">Method Overview</h2> -->
          <center>
          <img src="static/images/framework.png" alt="Framework" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              This paper proposes a method to perceive the state of the entire road network for MFM-based traffic scenarios. First, a structured semantic annotation approach is employed for video-image labeling, coupled with traffic video datasets to construct a multi-modal road traffic dataset. The videos and corresponding annotations are fed into the model. A video Q-Former extracts spatiotemporal dynamic features from the videos and aligns visual features with textual information. Finally, the aligned joint features, combined with prompt text, are input into a pre-trained LLM, which outputs traffic scene comprehension text. To further adapt the framework to roadside monitoring scenarios, we implement a traffic scenario-refined fine-tuning strategy using Low-Rank Adaptation (LoRA). Through end-to-end multi-modal alignment and lightweight optimization, this framework provides a high-precision, low-latency, and interpretable traffic scene understanding solution for road network state perception. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h3 class="title is-3" style="text-align: center; padding-bottom: 5px; color: rgb(63	172	38)"><bold>Performance Testing</bold></h3>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static\images\scene understanding.png" alt="Framework" class="center-image blend-img-background"/>
      <h2 class="subtitle has-text-centered">
        Time-Series-Aligned Description Generation for Urban Intersection Dynamics
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static\images\incident detection.png" alt="Framework" class="center-image blend-img-background"/>
      <h2 class="subtitle has-text-centered">
        Traffic Incident Detection and Description Generation (Congestion)
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static\images\incident detection1.png" alt="Framework" class="center-image blend-img-background"/>
      <h2 class="subtitle has-text-centered">
        Traffic Incident Detection and Description Generation (Construction)
      </h2>
    </div>
  </div>
</section>



<section class="hero is-light is-small">
  <div class="hero-body">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h3 class="title is-3" style="text-align: center; padding-bottom: 5px; color: rgb(63	172	38)"><bold>Sample Demonstration</bold></h3>
      </div>
    </div>
  </div>
  </div>
</section>

<style>
  /* 新增样式 */
  .code-container11 {
    max-width: 960px;        /* 控制最大宽度 */
    margin: 2rem auto;       /* 上下留白 + 水平居中 */
    background: #f8f9fa00;
  }
  
  .code-container11 pre {
    margin: 0;               /* 清除默认外边距 */
    font-size: 0.9em;        /* 调小字体 */
    line-height: 1.4;        /* 紧凑行高 */
    white-space: pre-wrap;   /* 保留换行同时允许自动换行 */
    word-break: break-all;   /* 强制换行策略 */
  }
  
  /* Bulma 框架适配 */
  .container.is-max-desktop .content11 {
    display: flex;
    justify-content: center; /* 水平居中 */
  }
  </style>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="level-set has-text-justified">
      <p>
        Here is an example from the dataset our research team has constructed – a sample sourced from The Tianjin Road Intersection Dataset. <b> Complete dataset will be made publicly available upon publication of the paper.</b>
      </p>
    </div>

    <!-- 修改1：给 hero-body 添加 flex 布局 -->
    <div class="hero-body is-flex is-flex-direction-column is-align-items-center">
      <!-- 修改2：限制视频最大宽度 -->
      <video 
        poster="" 
        id="tree" 
        autoplay 
        controls 
        muted 
        loop 
        height="100%"
        style="max-width: 80%; width: auto;" <!-- 控制视频宽度 -->
      >
        <source src="static/videos/Example.mp4" type="video/mp4">
      </video>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="level-set has-text-justified">
      <p>
        The annotations for the aforementioned example video are structured as follows: The annotation files consist of a list of dictionaries, with each entry adhering to the uniform format below: 
      </p>
    </div>
    
    <div class="container is-max-desktop content">
      <div class="code-container11">
        <pre><code>
[
  {
    "id": 0,
    "video": "datasets/example.mp4",
    "conversations": [
      {
        "from": "human",
        "value": "&lt;video&gt;\nClarify the contents of the displayed video with great detail."
      },
      {
        "from": "gpt",
        "value": "First, two black cars drove out of the intersection from bottom to top, and one black car drove out of the intersection from top to bottom. Next, the traffic lights turned from green to red, and a white car stopped outside the intersection at the bottom of the video. Then, multiple electric cars and cars drove through the intersection in an orderly manner. Finally, a black car turned left."
      }
    ]
  }
]
        </code></pre>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Declaration of competing interest: The authors declare that they have no known competing financial interests or personal relationships that could have
appeared to influence the work reported in this paper
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
    
</body>
</html>
